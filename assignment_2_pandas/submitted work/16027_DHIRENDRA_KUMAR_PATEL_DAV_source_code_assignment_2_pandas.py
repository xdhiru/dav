# -*- coding: utf-8 -*-
"""assignment_2_pandas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eRlJScuwH9hY9JcQ_hS72DQTGtEiIAui

<H1>DAV : Assignment 2 (Pandas)
<BR>Name : DHIRENDRA KUMAR PATEL
<BR>ROLL : 16027<H1>
"""

import pandas as pd
import numpy as np
src="/content/drive/MyDrive/Classroom/DAV sem 3/16027_DHIRENDRA_KUMAR_PATEL/assignment_2_pandas/csvdata"

"""---
<h3>Q1. Use a dataset of your choice from Open Data Portal (https:// data.gov.in/, UCI repository) . Load a Pandas dataframe with a selected dataset. Identify and count the missing values in a dataframe. Clean the data after removing noise as follows
<br>a) Detect the outliers and remove the rows having outliers</h3>
"""

df1=pd.read_csv(f"{src}/q1.csv")
df1

"""<h3>b) Drop duplicate rows.</h3>"""

df1.drop_duplicates(inplace=True)
df1

"""<h3>c) Identify the most positively correlated attributes and negatively correlated attributes</h3>"""

cmatrix=df1.corr(numeric_only=True)
print("Correlation Matrix :\n")
cmatrix[np.eye(len(cmatrix),dtype=bool)]=np.nan  #replace correlation = 1 with NaN
cmatrix

print("Maximum Correlated : \n",cmatrix.stack().idxmax()," : ", cmatrix.max().max())
print("\nMinimum Correlated : \n",cmatrix.stack().idxmin()," : ", cmatrix.min().min())

"""---
<h3>Q2. Given below is a dictionary having two keys ‘Boys’ and ‘Girls’ and having two lists of heights of five Boys
and Five Girls respectively as values associated with these keys Original dictionary of lists:
<br>{'Boys': [72, 68, 70, 69, 74], 'Girls': [63, 65, 69, 62, 61]}
<br><br>From the given dictionary of lists create the following list of dictionaries:
<br>[{'Boys': 72, 'Girls': 63}, {'Boys': 68, 'Girls': 65}, {'Boys': 70, 'Girls': 69}, {'Boys': 69, 'Girls': 62},{‘Boys’:74, ‘Girls’:61}]
<br><br>What are multiple ways to do it? Give at least 3 methods to achieve it? Explain each method as the comment of your code.
"""

d1={'Boys': [72, 68, 70, 69, 74], 'Girls': [63, 65, 69, 62, 61]}
df1=pd.DataFrame(d1)
def f(arow):      #apply function to DataFrame that takes each row and returns dictionary containing Boy-Girl pairs of that row, then store all returned dicts in a list using tolist()
  return {"Boys":arow["Boys"],"Girls":arow["Girls"]}
print(df1,"\n")
l1=df1.apply(f,axis=1).tolist()
print(l1)

l2=[]
for i in range(len(d1["Boys"])):   #iterate both lists in the dictionary, then make dictionary of the Boy-Girl index-wise pairs and append to a new list
  l2.append({"Boys":d1["Boys"][i],"Girls":d1["Girls"][i]})
print(l2)

l3=[]
for i,j in zip(d1["Boys"],d1["Girls"]):   #use zip to map the dictionary lists and generate zipped Boy-Girl pairs based on index of list, then make dictionary of the Boy-Girl pairs and append to a new list
  l3.append({"Boys":i,"Girls":j})
print(l3)

l4=df1.to_dict(orient="records")  #use to_dict function of DataFrame with orientation=records which gives list of dictionary like [{column1 : value1, column2 : value1}, {column1 : value2, column2 : value2}]
print(l4)

"""---
<H3>Q3.Create a dataframe having at least 5 columns and 100 rows to store numeric data generated using a random function. Replace 25% of the values by null values whose index positions are generated using random function. Do the following:
</H3>

"""

df3=pd.DataFrame(np.random.randint(100,size=(200,10)))
df3

nancount=0
nanneeded=df3.size*0.25
while nancount<nanneeded:
  x=np.random.randint(0,df3.shape[0])
  y=np.random.randint(0,df3.shape[1])
  if not pd.isnull(df3.iloc[x,y]):
    df3.iloc[x,y]=np.nan
    nancount+=1
df3

"""<h3>a. Identify and count missing values in a dataframe.</h3>"""

count = df3.isnull().sum().sum()
print("Total Null Values : ", count)

"""<h3>b. Drop the column having more than 5 null values.</h3>"""

not_more_than_5_nulls = df3.isnull().sum()<=5
df3.loc[:,not_more_than_5_nulls]
#this deletes all columns as the question has asked for minimum 100 rows, 5 columns with 25% of all values to be NaN. Meaning, all the columns have more than 5 null values and are hence deleted.

"""<h3>c. Identify the row label having maximum of the sum of all values in a row and drop that row.</h3>"""

max_rowsum=df3.sum(axis=1).max()
print("Maximum Sum in a Row is : ",max_rowsum)
no_drop_condition=(df3.sum(axis=1) != max_rowsum)
print("Dataframe after dropping max sum row :")
df3.loc[no_drop_condition,:]

"""<h3>d. Sort the data frame on the basis of the first column.</h3>"""

df3.sort_values(by=[0],axis=0)

"""<h3>e. Remove all duplicates from the first column.</h3>"""

df3.drop_duplicates(subset=[0])

"""<h3>f. Find the correlation between first and second column and covariance between second and third column. g. Detect the outliers and remove the rows having outliers.</h3>

"""

print("Correlation between first and second column : ",df3[0].corr(df3[1]))
print("Covariance between second and third column : ",df3[1].cov(df3[2]))

"""<h3>g. Detect the outliers and remove the rows having outliers.</h3>"""

z_score_threshold = 5
z_scores = (df3 - df3.mean()) / df3.std()
print("DataFrame after removing rows with outliers :\n")
df3[(z_scores.abs() < z_score_threshold).all(axis=1)]

"""<h3>h. Discretize second column and create 5 bins</h3>"""

pd.cut(df3[1], bins=5)

"""---
<h3>Q4.Consider two excel files having attendance of a workshop’s participants for two days. Each file has three fields ‘Name’, ‘Time of joining’, duration (in minutes) where names are unique within a file. Note that duration may take one of three values (30, 40, 50) only. Import the data into two dataframes and do the
following:</h3>

"""

ws1=pd.read_csv(f"{src}/workshop1.csv")
ws2=pd.read_csv(f"{src}/workshop2.csv")
print("Workshop 1 :\n",ws1)
print("\nWorkshop 2 :\n",ws2)

"""<h3>a. Perform merging of the two dataframes to find the names of students who had attended the workshop on both days.</h3>"""

ws3=pd.merge(ws1,ws2,on="Name",how="inner")
print(ws3)

"""
<h3>b. Find names of all students who have attended workshop on either of the days.</h3>"""

ws4=pd.merge(ws1,ws2,on="Name",how="outer")
print(ws4)

"""
<h3>c. Merge two data frames row-wise and find the total number of records in the data frame.</h3>"""

ws5=pd.merge(ws1,ws2,on="Name",how="outer")
ws5.count(axis=0)

"""
<h3>d. Merge two data frames and use two columns names and duration as multi-row indexes. Generate descriptive statistics for this multi-index.</h3>"""

ws6 = pd.merge(ws1, ws2, on=['Name', 'Duration'], how='outer')
ws6.set_index(['Name', 'Duration'], inplace=True)
statistics = ws6.describe()
print("Merged DataFrame with Multi-Index :\n",ws6)
print("\nDescriptive Statistics for Multi-Index :\n",statistics)

"""---
<h3>Q5.Consider a data frame containing data about students i.e. name, gender and passing division:</h3>
"""

stdf1=pd.read_csv(f"{src}/students.csv")
stdf1

"""
<h3>a. Perform one hot encoding of the last two columns of categorical data using the get_dummies() function.</h3>"""

pd.get_dummies(stdf1, columns=["Gender","Pass_Division"])

"""<h3>b. Sort this data frame on the “Birth Month” column (i.e. January to December). (Hint: Convert Month to Categorical.)</h3>"""

stdf1['Birth_Month'] = pd.Categorical(stdf1['Birth_Month'], categories=['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'], ordered=True)
stdf1.sort_values(by='Birth_Month')

"""---
<h3>Q6.Consider the following data frame containing a family name, gender of the family member and her/his monthly income in each record. Write a program in Python using Pandas to perform the following:</h3>
"""

fam1=pd.read_csv(f"{src}/family.csv")
fam1

"""<h3>A. Calculate and display familywise gross monthly income.</h3>"""

print("Familywise gross monthly income : ")
fam1.groupby('FamilyName')['MonthlyIncome(Rs.)'].sum()

"""<h3>B. Display the highest and lowest monthly income for each family name.</h3>"""

print("Highest income for each family :")
fam1.groupby('FamilyName')['MonthlyIncome(Rs.)'].max()

print("Lowest income for each family :")
fam1.groupby('FamilyName')['MonthlyIncome(Rs.)'].min()

"""<h3>C. Calculate and display monthly income of all members earning income less than Rs. 80000.00.</h3>"""

print("Monthly income of all members earning less than Rs. 80000.00 :")
fam1[fam1['MonthlyIncome(Rs.)']<80000]

"""<h3>D. Calculate and display the average monthly income of the female members in the Shah family.</h3>"""

print("Average monthly income of Shah family female members :", ( fam1[(fam1["FamilyName"]=="Shah") & (fam1["Gender"]=="Female")] ) ["MonthlyIncome(Rs.)"].mean() )

"""<h3>E. Calculate and display monthly income of all members with income greater than Rs. 60000.00.</h3>"""

print("Monthly income of all members with income greater than Rs. 60000.00 :")
fam1[fam1['MonthlyIncome(Rs.)']>60000]

"""<h3>F. Display total number of females along with their average monthly income.</h3>"""

female_count = fam1[fam1['Gender'] == 'Female'].groupby('FamilyName')['Gender'].count()
monthly_female_income =fam1[fam1['Gender']=="Female"].groupby("FamilyName").mean(numeric_only=True)
pd.merge(female_count, monthly_female_income, on="FamilyName")

"""<h3>G. Delete rows with Monthly income less than the average income of all members</h3>"""

avg_income = fam1["MonthlyIncome(Rs.)"].mean()
print("Average income of all members :", avg_income)
print("After deleting rows with Monthly income less than the average income :")
fam1[fam1.loc[:,"MonthlyIncome(Rs.)"]>avg_income]

"""---
<h3>Q7.Using the parsed.csv file, complete the following exercises to practise your pandas skills:
"""

par1=pd.read_csv(f"{src}/parsed.csv")
par1

"""<h3>a. Find the 95th percentile of earthquake magnitude in Japan using the magType of 'mb'.</h3>"""

japan_eq=par1[(par1['parsed_place'] == 'Japan') & (par1['magType'] == 'mb')]
print("95th percentile of earthquake magnitude in Japan using the magType of 'mb' : ")
japan_eq['mag'].quantile(0.95)

"""
<h3>b. Find the percentage of earthquakes in Indonesia that were coupled with tsunamis.</h3>"""

indonesia_eq = par1[par1['parsed_place'] == 'Indonesia']
indonesia_eq_count = len(indonesia_eq)
tsunami_eq = len(indonesia_eq[indonesia_eq['tsunami'] == 1])
print("Percentage of earthquakes in Indonesia that were coupled with tsunamis :")
(tsunami_eq/indonesia_eq_count)*100

"""
<h3>c. Get summary statistics for earthquakes in Nevada.</h3>"""

nevada_eq = par1[par1['parsed_place'] == 'Nevada']
nevada_eq.describe()

"""
<h3>d. Add a column to the dataframe indicating whether or not the earthquake happened in a country or US state that is on the Ring of Fire. Use Bolivia, Chile, Ecuador, Peru, Costa Rica, Guatemala, Mexico (be careful not to select New Mexico), Japan, Philippines, Indonesia, New Zealand, Antarctica (look for Antarctic), Canada, Fiji, Alaska, Washington, California, Russia, Taiwan,
Tonga, and Kermadec Islands.</h3>"""

rings_of_fire = ['Bolivia', 'Chile', 'Ecuador', 'Peru', 'Costa Rica', 'Guatemala', 'Mexico', 'Japan', 'Philippines', 'Indonesia', 'New Zealand', 'Antarctica', 'Canada', 'Fiji', 'Alaska', 'Washington', 'California', 'Russia', 'Taiwan', 'Tonga', 'Kermadec Islands']
par1['Exists in Ring of fire?'] = par1['parsed_place'].isin(rings_of_fire)
par1

"""
<h3>e. Calculate the number of earthquakes in the Ring of Fire locations and the number outside them.</h3>"""

print("Number of earthquakes in the Ring of Fire locations :",len(par1[par1['Exists in Ring of fire?'] == True]))
print("Number of earthquakes outside the Ring of Fire locations :",len(par1[par1['Exists in Ring of fire?'] == False]))

"""
<h3>f. Find the tsunami count along the Ring of Fire.</h3>"""

print("Tsunami count along the Ring of Fire locations:", len(par1[(par1['Exists in Ring of fire?']==True) & (par1['tsunami']==1)]))

"""---
<h3>Q8.Using the CSV files in the earthquakes.csv folder, Write a program in Python using Pandas to perform the following:</h3>
"""

eq1=pd.read_csv(f"{src}/earthquakes.csv")
eq1

"""<h3>a. With the earthquakes.csv file, select all the earthquakes in Japan with a magType of mb and a magnitude of 4.9 or greater.</h3>"""

eq1[(eq1['parsed_place']=='Japan') & (eq1['magType']=='mb') & (eq1['mag']>=4.9)]

"""
<h3>b. Create bins for each full number of magnitude (for example, the first bin is 0-1, the second is 1-2, and so on) with a magType of ml and count how many are in each bin.</h3>"""

ml_eq = eq1[eq1['magType'] == 'ml']
bins = list(range(int(ml_eq['mag'].min()), int(ml_eq['mag'].max()) + 1))
binned_eq = pd.cut(ml_eq['mag'], bins, right=False)
binned_counts = binned_eq.value_counts().sort_index()
print(binned_counts)

"""<h3>c. Build a crosstab with the earthquake data between the tsunami column and the magType column. Rather than showing the frequency count, show the maximum magnitude that was observed for each combination. Put the magType along the columns.</h3>"""

pd.crosstab(eq1['tsunami'], eq1['magType'], values=eq1['mag'], aggfunc='max')

"""---
<h3>Q9.Using the faang.csv file, group by the ticker and resample to monthly frequency. Make the following aggregations:</h3>
"""

fng1=pd.read_csv(f"{src}/faang.csv", parse_dates=['date'],index_col="date")
fng1_resampled=fng1.groupby('ticker').resample('M')

"""<h3>a. Mean of the opening price</h3>"""

fng1_resampled['open'].mean()

"""<h3>b. Maximum of the high price</h3>"""

fng1_resampled['high'].max()

"""<h3>c. Minimum of the low price</h3>"""

fng1_resampled['low'].min()

"""<h3>d. Mean of the closing price</h3>"""

fng1_resampled['close'].mean()

"""<h3>e. Sum of the volume traded</h3>"""

fng1_resampled['volume'].sum()